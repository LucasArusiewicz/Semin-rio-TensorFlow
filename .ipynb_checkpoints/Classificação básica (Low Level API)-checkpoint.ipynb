{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega data set\n",
    "from sklearn import datasets\n",
    "\n",
    "# Carrega funções para pré-processamento\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Carrega função para auxiliar no treinamento\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carrega função para calcular presição da rede\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carrega TensorFlow e Numpy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Criar Enconder de saidas desejadas\n",
    "onehot = OneHotEncoder(categorical_features = [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando dados para o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "# Carrega dados de entrada para rede\n",
    "X = iris.data\n",
    "\n",
    "# Normaliza valores\n",
    "scaler_x = StandardScaler()\n",
    "X = scaler_x.fit_transform(X)\n",
    "\n",
    "# Carrega resulta esperado\n",
    "y = iris.target\n",
    "\n",
    "# Reestrutura valores para de (150,) para (150,1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Cria lista de saídas desejadas\n",
    "y = onehot.fit_transform(y).toarray()\n",
    "\n",
    "# Divide Data set para treinamento de testes de validação\n",
    "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo Topologia da Rede\n",
    "\n",
    "![alt text](img/topologia.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantidade de neurônios na camada de entrada (4)\n",
    "neuronios_entrada = X.shape[1]\n",
    "\n",
    "# Define quantidade de neurônios na camada oculta (4)\n",
    "neuronios_oculta = int(np.ceil((X.shape[1] + y.shape[1]) / 2))\n",
    "\n",
    "# Define quantidade de neurônios na cada de saida (3)\n",
    "neuronios_saida = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo pesos\n",
    "Como valor de inicialização para os pesos foi utilizado distribuição normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "W = {'oculta': tf.Variable(tf.random_normal([neuronios_entrada, neuronios_oculta])),\n",
    "     'saida': tf.Variable(tf.random_normal([neuronios_oculta, neuronios_saida]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo Bias\n",
    "Valores inciados igualmente os pessos, utilizando a função de distribuição normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {'oculta': tf.Variable(tf.random_normal([neuronios_oculta])),\n",
    "     'saida': tf.Variable(tf.random_normal([neuronios_saida]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando placeholder\n",
    "Para inicialização dos placeholder é necessário passar o tipo e \"shape\" dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xph = tf.placeholder('float', [None, neuronios_entrada])\n",
    "yph = tf.placeholder('float', [None, neuronios_saida])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, W, bias):\n",
    "    camada_oculta = tf.add(tf.matmul(x, W['oculta']), bias['oculta'])\n",
    "    camada_oculta_ativacao = tf.nn.relu(camada_oculta)\n",
    "    camada_saida = tf.add(tf.matmul(camada_oculta_ativacao, W['saida']), b['saida'])\n",
    "    return camada_saida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = multilayer_perceptron(xph, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando erro e ajustando pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "erro = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = modelo, labels = yph))\n",
    "otimizador = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(erro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando Pacotes de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "batch_total = int(len(X_treinamento) / batch_size)\n",
    "X_batches = np.array_split(X_treinamento, batch_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efetuando treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época: 0 erro: 2.3251169644869294\n",
      "Época: 100 erro: 1.2902218103408813\n",
      "Época: 200 erro: 0.8350172982766079\n",
      "Época: 300 erro: 0.6420658207856691\n",
      "Época: 400 erro: 0.5285458175035624\n",
      "Época: 500 erro: 0.44584519358781666\n",
      "Época: 600 erro: 0.37868732328598315\n",
      "Época: 700 erro: 0.3237523539708211\n",
      "Época: 800 erro: 0.2790547712491109\n",
      "Época: 900 erro: 0.24176890804217413\n",
      "Época: 1000 erro: 0.2106014919968752\n",
      "Época: 1100 erro: 0.1846508320707541\n",
      "Época: 1200 erro: 0.15823534016425794\n",
      "Época: 1300 erro: 0.13790731246654808\n",
      "Época: 1400 erro: 0.12197066556948884\n",
      "Época: 1500 erro: 0.10934968321369241\n",
      "Época: 1600 erro: 0.09940474279797994\n",
      "Época: 1700 erro: 0.09143831580877305\n",
      "Época: 1800 erro: 0.08492738839525442\n",
      "Época: 1900 erro: 0.07952436331946117\n",
      "Época: 2000 erro: 0.07498925064618772\n",
      "Época: 2100 erro: 0.07114486940778218\n",
      "Época: 2200 erro: 0.06785663303274374\n",
      "Época: 2300 erro: 0.06502926535904408\n",
      "Época: 2400 erro: 0.06257026656888999\n",
      "Época: 2500 erro: 0.060428374232007906\n",
      "Época: 2600 erro: 0.05854634396158731\n",
      "Época: 2700 erro: 0.0568860716258104\n",
      "Época: 2800 erro: 0.055415688727337584\n",
      "Época: 2900 erro: 0.054108947228926875\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Inicializa valores das variaveis\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoca in range(3000):\n",
    "        erro_medio = 0.0\n",
    "        \n",
    "        # Cria pacotes de treinamento\n",
    "        batch_total = int(len(X_treinamento) / batch_size)\n",
    "        X_batches = np.array_split(X_treinamento, batch_total)\n",
    "        y_batches = np.array_split(y_treinamento, batch_total)\n",
    "        \n",
    "        # Para cada pacote de treinamento\n",
    "        for i in range(batch_total):\n",
    "            \n",
    "            # Pega dados do pacote atual\n",
    "            X_batch, y_batch = X_batches[i], y_batches[i]\n",
    "            \n",
    "            # Joga dados para rede aprender\n",
    "            _, custo = sess.run([otimizador, erro], feed_dict = {xph: X_batch, yph: y_batch})\n",
    "            \n",
    "            # Computa erro médio\n",
    "            erro_medio += custo / batch_total\n",
    "            \n",
    "        # Para cada 100 epocas exibe progresso de aprendizado\n",
    "        if epoca % 100 == 0:\n",
    "            print('Época: ' + str((epoca)) + ' erro: ' + str(erro_medio))\n",
    "            \n",
    "    # Coleta valores dos pesos e bias da rede\n",
    "    W_final, b_final = sess.run([W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando previsões\n",
    "Devemos utilizar a mesma arquitetura e topologia para realizar as previsões, só que desta vez passando os valores do pesos e bias, ajustados pelo treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes_teste = multilayer_perceptron(xph, W_final, b_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos executar somente o processo de feedforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 0, 2, 1, 0, 2, 0, 2, 0, 2, 1, 2, 0, 0, 2, 2, 0, 0, 0,\n",
       "       2, 1, 0, 0, 2, 0, 2, 0, 0, 2, 2, 0, 1, 2, 1, 2, 0, 1, 0, 2, 2, 0,\n",
       "       2], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    camada_oculta = sess.run(previsoes_teste, feed_dict = {xph: X_teste})\n",
    "    camada_saida = sess.run(tf.nn.softmax(camada_oculta))\n",
    "    resultado_indices = sess.run(tf.argmax(camada_saida, 1))\n",
    "resultado_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 0, 2, 1, 0, 2, 0, 2, 0, 2, 1, 1, 0, 0, 1, 2, 0, 0, 0,\n",
       "       2, 1, 0, 0, 2, 0, 2, 0, 0, 1, 2, 0, 1, 2, 1, 1, 0, 1, 0, 2, 2, 0,\n",
       "       0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste2 = np.argmax(y_teste, 1)\n",
    "y_teste2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxa_acerto = accuracy_score(y_teste2, resultado_indices)\n",
    "taxa_acerto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
